\documentclass{beamer}

\usepackage{xparse}
\usepackage{xfrac}
\usepackage[siunitx, american]{circuitikz}
\usepackage{hyperref}

\NewDocumentCommand{\differential}{}{\mathrm d}

\NewDocumentCommand{\vocab}{m}{\textbf{#1}}


% Use this macro as follows:
%   d/dt => \ddt{}
%   dx/dt => \ddt{x}
%   df/dx => \ddt{f}[x]
%   d^2 f/d x^2 => \ddt[2]{f}[x]
%   \partial f/\partial x => \ddt[][\partial]{f}[x]
\NewDocumentCommand{\ddt}{s o  O{\differential} m O{t}}{
  \IfBooleanTF {#1}{%
    \IfNoValueTF{#2}{%
      \sfrac{#5 #4}{#5 #3}
    }{%
      \sfrac{#3^{#2} #4}{#3 #5^{#2}}
    }
  }{%
    \IfNoValueTF{#2}{%
      \frac{#3 #4}{#3 #5}
    }{%
      \frac{#3^{#2} #4}{#3 #5^{#2}}
    }
  }
}

\usetheme{hutch-pdflatex}

\title{EE16B --- Midterm 2 Review}
\author{George Higgins Hutchinson, Parth Nobel, \textit{et al.}}
\date{\today}

\begin{document}
	\begin{frame}
		\titlepage
	\end{frame}

	\begin{frame}{Disclaimer}
	This is an unofficial review session and HKN is not affiliated with this course. All of the topics we're reviewing will reflect the material you have covered, our experiences in EE16B, and past exams. We make no promise that what we cover will necessarily reflect the content of this midterm. While some course staff members may be among the presenters, this review session is still not official.
	\vspace{1em}
	
	This is licensed under the Creative Commons CC BY-SA: feel free to share and edit, as long as you credit us and keep the license. For more information, visit \\ \small{\url{https://creativecommons.org/licenses/by-sa/4.0/deed.en_US}}
	
	\end{frame}

	\section{Singular Value Decomposition}

    \begin{frame}{SVD Theorem}
Any matrix $A \in \mathbb{R}^{m \times n}$ can be decomposed into the product of three matrices
		\begin{align*}
			A &= U \Sigma V^T \\
			U &: m \times m \\
			\Sigma &: m \times n \\
			V^T &: n \times n
		\end{align*}
		Such that $U, V$ are unitary matrices and $\Sigma$ only has nonnegative values along its main diagonal.

    \end{frame}

    \begin{frame}{SVD: Compact Form}
        We can also express the SVD as
		\begin{align*}
			A &= \mathcal{U} S \mathcal{V}^T \\
			\mathcal{U} &: m \times r \\
			S &: r \times r \\
			\mathcal{V}^T &: r \times n
		\end{align*}
		where $r$ is the rank of $A$. The compact form matrices maintain properties of the original matrices, but have entries removed whenever they correspond to zero singular values.

    \end{frame}


    \begin{frame}{SVD: Outer Product Form}
		Lastly, we can express
		\[ A = \sum_{i = 1}^r \sigma_i \vec{u}_i \vec{v}_i^T \]
		where $\vec{u}_i, \vec{v}_i$ are the columns of $U, V$, respectively, and $\sigma_i$ are corresponding diagonal entry of the matrix $\Sigma$
    \end{frame}

    \begin{frame}{Computing SVD with $A^T A$}
		\begin{align*}
			A^T A &= U \Sigma V^T V \Sigma^T U^T \\
			&= U \Sigma^2 U^T
		\end{align*}
		This is an eigen decomposition since $\Sigma^2$ is diagonal and $U^{-1} = U^T$. Thus solving for the eigenvalues and eigenvectors of $A^T A$ give $\lambda_i = \sigma_i^2$ with eigenvectors which correspond to the right singular vectors. We need to sort by decreasing $\sigma_i$. \\

        \alert{Side note:} $\Sigma^T \Sigma$ is not actually equal to $\Sigma^2$, but the former product yields a matrix with singular values squared on the diagonal entries, hence we call it $\Sigma^2$
    \end{frame}

    \begin{frame}{Computing SVD with $A^T A$}
Given a right singular vector $\vec{v}_i$ which we found from the previous part, we can apply it
		\begin{align*}
			A \vec{v}_i &= \left( \sum_{k = 1}^r \sigma_k \vec{u}_k \vec{v}_k^T \right) \vec{v}_i \\
			&= \sum_{k = 1}^r \sigma_k \vec{u}_k \vec{v}_k^T \vec{i} \\
			&= \sigma_i \vec{u}_i \\
			\vec{u}_i &= \frac{1}{\sigma_i} A \vec{v}_i
		\end{align*}
    \end{frame}

    \begin{frame}{Computing SVD with $A A^T$}
        Similar calculations yield $\sigma_i = \sqrt{\lambda_i}$ of $A A^T$ with eigenvectors as left singular vectors, and $\vec{v}_i = \frac{1}{\sigma_i} A^T \vec{u}_i$
    \end{frame}

    \begin{frame}{Intepretation of SVD}
    \begin{itemize}
    	\item Unitary matrices act as rotation in a given space. A diagonal matrix stretches in a given coordinate space.
    	
    	\item \href{https://en.wikipedia.org/wiki/File:Singular_value_decomposition.gif}{SVD visualization (open in browser)}
    \end{itemize}
		

		

    \end{frame}

    \begin{frame}{Intepretation of SVD}
		For a product $A \vec{x}$, we can decompose every vector $\vec{x}$ into a linear combination of right singular vectors
		\[ \vec x = \sum_{i = 1}^n \alpha_i \vec{v}_i \]
		Thus, we can see exactly which parts of $\vec{x}$ affect the output.
    \end{frame}


	\begin{frame}{Compression of Low-Rank Matrices}
	\begin{itemize}[<+->]
	\item Suppose I had a matrix $A \in \mathbb{R}^{m \times n}$ with $m, n >> rank(A)$. How could I more efficiently store $A$ and compute products like $A \vec{x}$?
	\vspace{2em}
	\item With the SVD, we only have to save $r$ set of two vectors and a scalar, which saves us a lot of space if the rank is small with respect to the matrix. Also, less computation is carried out if we represent the matrix as the outer product form.
	\end{itemize}
	\end{frame}

	\section{Principle Component Analysis}

	\begin{frame}{PCA}
	PCA is a linear dimensionality reduction tool. Given data $\vec{x}_i \in \mathbb{R}^d$, we can create a mapping $T : \mathbb{R}^d \rightarrow \mathbb{R}^{d'}, d' < d$ such that the variance in the dataset is still captured
	\end{frame}

	\begin{frame}{PCA --- Computation}
		\begin{enumerate}[<+->]
			\item Store data row-major in $A \in \mathbb{R}^{n \times d}$
			\item De-mean $A$
			\item Take SVD: $A = U \Sigma V^T$
			\item Create $V_{d'} \in \mathbb{R}^{n \times d'}$ from vectors of $V$ corresponding to $d'$ greatest signular values
			\item To project data into the representative subspace: $T(x) := V_{d'}^T x$
		\end{enumerate}
	\end{frame}

	\begin{frame}{PCA: computation}
	The mapping $T$ can then be expressed as
	\[ T(\vec{x}) = V_k^T \vec{x} \]
	If we apply this transformation onto the entire dataset (which has row vectors), we can say
	\[ T(A) = B = A V_k \]
	where $B \in \mathbb{R}^{n \times k}$
	\end{frame}
	\begin{frame}{PCA: computation}
	If we were to show the projected vectors in the original space, we can multiply back with the projection vectors
	\[ A' = B V_k^T \]
	\end{frame}

	\section{Discretization}
	
	\begin{frame}{Discretization: Q1}
	Note: this section follows hw8 q1 almost exactly. Suppose we have a scalar system
	\[ \frac{d}{dt} x(t) = \alpha x + \vec{\beta}^T \vec u(t) \]
	and we apply a constant input $\vec{u}_n$ for times $t \in [nT, (n + 1)T)$ for some $T > 0$. Given $x(nT)$ solve the differential equation
	\end{frame}\begin{frame}{Discretization: Q1 Sol}
	From $t = nT$ to $t = (n + 1)T$, $\vec{\beta}^T \vec{u}$ is a constant scalar. Thus, we can solve this like a normal differential equation. Let $x = x' - \frac{\vec \beta^T \vec u}{\alpha}$. Then
	\begin{align*}
	\frac{d}{dt} x(t) &= \alpha (x' - \frac{\vec \beta^T \vec u}{\alpha}) + \vec{\beta}^T \vec u(t) \\
	&= \alpha x' \\
	x' &= A e^{\alpha (x - nT)} \\
	x + \frac{\vec \beta^T \vec u}{\alpha} &= A e^{\alpha (x - nT)} \\
	x &= A e^{\alpha (x - nT)} - \frac{\vec \beta^T \vec u}{\alpha}
	\end{align*}
	At which point we can use our initial condition to get
	\begin{align*}
	x(nT) &= A - \frac{\vec \beta^T \vec u}{\alpha} \\
	A &= x(nT) + \frac{\vec \beta^T \vec u}{\alpha} \\
	x &= \left( x(nT) + \frac{\vec \beta^T \vec u}{\alpha} \right) e^{\alpha (t - nT)} - \frac{\vec \beta^T \vec u}{\alpha}
	\end{align*}
	\end{frame}\begin{frame}{Discretization: Q2}
	Using the differential equation derived from question 1, create a discrete-time system to model the continuous time. In other words, if $x[n] = x(nT), \vec{u}[n] = \vec{u}(nT)$, find a relation such that
	\[ x[n + 1] = A_d x[n] + B_d \vec{u}[n] \]
	\end{frame}\begin{frame}{Discretization: Q2 Sol}
	We can solve the previous solution for $x((n + 1)T)$
	\begin{align*}
	x((n + 1)T) &= \left( x(nT) + \frac{\vec \beta^T \vec u(nT)}{\alpha} \right) e^{\alpha ((n + 1)T - nT)} - \frac{\vec \beta^T \vec u(nT)}{\alpha} \\
	x[n + 1] &= e^{\alpha T} x[n] + \frac{e^{\alpha T} - 1}{\alpha} \vec{\beta}^T \vec{u}[n] \\
	\end{align*}
	We see that $A_d = e^{\alpha T}, B_d = ((e^{\alpha T} - 1) / \alpha) \vec{\beta}^T$
	\end{frame}\begin{frame}{Discretization: Q3}
	Instead of a scalar, we instead have a diagonal matrix $A$ such that
	\[ \frac{d}{dt} \vec{x} = A \vec{x} + B \vec{u} \]
	Discretize this system in the same was as Q2.
	\end{frame}\begin{frame}{Discretiziation: Q3 Sol}
	Expanding the original system out line-by-line gives
	\[ \frac{d}{dt} x_i = a_i x_i + b_i \vec{u}_i \]
	where $x_i$ is the $i$th variable of $\vec{x}$, $a_i$ is the diagonal entry of $A$, and $b_i$ is the row of $B$.
	\end{frame}
	\begin{frame}{Discretization: Generic Matrix}
	
	Math not shown, but we can perform a change of basis from our original space to our diagonal space, and then apply the results of the previous part.
	
	\end{frame}
	

%    \begin{frame}{Oscillators}
%        Consider a DAE \[
%            \vec f(\vec x(t)) + \ddt{} \vec q(\vec x(t)) = \vec 0
%        \]
%        If there exists a non-constant \(\vec x_p(t)\) such that \[
%            \exists T> 0,\forall t, \vec x_p(t) = \vec x_p(t + T)
%        \] and \(\vec x_p(t)\) solves the DAE, we say that the DAE describes an \vocab{Oscillator} or an oscillatory system.
%    \end{frame}
%
%    \begin{frame}{An Oscillatory Problem}
%        \begin{center}
%        \begin{tikzpicture}[scale=2]
%            \draw (0, 0) to (0.75, 0) to (0.75, 0.0) node[ground] {} to (0.75, 0) to  (1.5, 0) to [C, l=2<\micro\farad>] (1.5, 1) to (0, 1) to[L, l=2<\henry>] (0, 0);
%            %\node [ground] (0.75, -0.1) {};
%        \end{tikzpicture}
%        \end{center}
%        \begin{enumerate}
%            \item<1-> Write out the DAE.
%                \visible<2->{ \color{foundersrock}
%                    \[
%                            \ddt{}
%                            \begin{bmatrix}
%                                v \\ i
%                            \end{bmatrix}
%                            =
%                            \begin{bmatrix}
%                                0 & -\sfrac{1}{2} \\
%                                \sfrac{10^6}{2} & 0 \\
%                            \end{bmatrix}
%                            \begin{bmatrix}
%                                v \\ i
%                            \end{bmatrix}
%                        \]
%            }
%            \item<1-> Show that this circuit is an oscillator.
%                \visible<3->{ \color{foundersrock}
%                    \[
%                            \vec x_p(t) = \begin{bmatrix}
%                                \cos(\sfrac{10^3}{2}{t}) \\
%                                10^3\sin(\sfrac{10^3}{2} t) \\
%                            \end{bmatrix}
%                        \]
%            }
%            \item<1-> Find its period.
%                \visible<4->{\color{foundersrock}
%                    \[
%                        T = \frac{4\pi}{10^3}\,\si{\s}
%                    \]
%            }
%        \end{enumerate}
%    \end{frame}

\end{document}
